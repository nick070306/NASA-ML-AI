{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6510bff7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prophet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprophet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prophet\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'prophet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import io\n",
    "from ollama import generate\n",
    "\n",
    "class CSVAnalyst:\n",
    "    def __init__(self):\n",
    "        self.df = None\n",
    "        self.analysis_report = \"\"\n",
    "        self.emergency_level = None\n",
    "        self.is_time_series = False\n",
    "        self.date_column = None\n",
    "        self.context_lists = {\n",
    "            'lower_is_better': {\"sugars\", \"calories\", \"sodium\", \"pollution\", \"errors\", \"temperature\"},\n",
    "            'higher_is_better': {\"sales\", \"profit\", \"ratings\", \"performance\", \"growth\", \"speed\"},\n",
    "            'neutral': {\"pressure\", \"ph\", \"voltage\", \"balance\"}\n",
    "        }\n",
    "        self.learned_attributes = {}\n",
    "\n",
    "    def load_csv(self, file_path):\n",
    "        \"\"\"Load CSV file into DataFrame with time series detection\"\"\"\n",
    "        try:\n",
    "            self.df = pd.read_csv(file_path)\n",
    "            self._basic_analysis()\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            return f\"Error loading CSV: {str(e)}\"\n",
    "\n",
    "    def _basic_analysis(self):\n",
    "        \"\"\"Perform initial data analysis with time series detection\"\"\"\n",
    "        analysis = []\n",
    "        self.is_time_series = False\n",
    "        \n",
    "        # Detect time series data\n",
    "        for col in self.df.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(self.df[col]):\n",
    "                self.is_time_series = True\n",
    "                self.date_column = col\n",
    "                self.df[col] = pd.to_datetime(self.df[col])\n",
    "                break\n",
    "\n",
    "        analysis.append(f\"Time Series Detected: {self.is_time_series}\")\n",
    "        if self.is_time_series:\n",
    "            analysis.append(f\"Date Column: {self.date_column}\")\n",
    "            analysis.append(f\"Date Range: {self.df[self.date_column].min()} to {self.df[self.date_column].max()}\")\n",
    "\n",
    "        # Add remaining analysis\n",
    "        analysis.extend([\n",
    "            f\"\\nDataset Shape: {self.df.shape}\",\n",
    "            \"\\nColumn Types:\",\n",
    "            self.df.dtypes.to_string(),\n",
    "            \"\\nMissing Values:\",\n",
    "            self.df.isnull().sum().to_string(),\n",
    "            \"\\nBasic Statistics:\",\n",
    "            self.df.describe(include='all').to_string()\n",
    "        ])\n",
    "        self.analysis_report = \"\\n\".join(analysis)\n",
    "\n",
    "    def generate_plots(self):\n",
    "        \"\"\"Generate appropriate visualizations for data type\"\"\"\n",
    "        plots = {}\n",
    "        \n",
    "        if self.is_time_series:\n",
    "            # Time series plots\n",
    "            numeric_cols = self.df.select_dtypes(include=np.number).columns\n",
    "            for col in numeric_cols:\n",
    "                plt.figure()\n",
    "                sns.lineplot(x=self.df[self.date_column], y=self.df[col])\n",
    "                plt.title(f'{col} over Time')\n",
    "                buf = io.BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                plots[f'timeseries_{col}'] = buf.getvalue()\n",
    "                plt.close()\n",
    "        else:\n",
    "            # Non-time series plots\n",
    "            numeric_cols = self.df.select_dtypes(include=np.number).columns\n",
    "            for col in numeric_cols:\n",
    "                # Histogram\n",
    "                plt.figure()\n",
    "                sns.histplot(self.df[col])\n",
    "                plt.title(f'Distribution of {col}')\n",
    "                buf = io.BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                plots[f'hist_{col}'] = buf.getvalue()\n",
    "                plt.close()\n",
    "\n",
    "                # Boxplot\n",
    "                plt.figure()\n",
    "                sns.boxplot(y=self.df[col])\n",
    "                plt.title(f'{col} Distribution')\n",
    "                buf = io.BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                plots[f'box_{col}'] = buf.getvalue()\n",
    "                plt.close()\n",
    "\n",
    "            # Correlation heatmap if multiple numeric columns\n",
    "            if len(numeric_cols) > 1:\n",
    "                plt.figure(figsize=(10, 8))\n",
    "                sns.heatmap(self.df[numeric_cols].corr(), annot=True)\n",
    "                plt.title('Feature Correlation Matrix')\n",
    "                buf = io.BytesIO()\n",
    "                plt.savefig(buf, format='png')\n",
    "                plots['correlation'] = buf.getvalue()\n",
    "                plt.close()\n",
    "\n",
    "        return plots\n",
    "\n",
    "    def calculate_emergency_level(self, target_col, n_days=7, n_samples=30, \n",
    "                                user_avg=None, sensitivity=0.1,\n",
    "                                auto_detect_direction=True):\n",
    "        \"\"\"\n",
    "        Universal emergency level calculation that adapts to data type\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if target_col not in self.df.columns:\n",
    "                raise ValueError(f\"Column '{target_col}' not found\")\n",
    "\n",
    "            # Get appropriate recent data\n",
    "            if self.is_time_series:\n",
    "                recent_data = self.df.set_index(self.date_column)[target_col].last(f'{n_days}D')\n",
    "            else:\n",
    "                recent_data = self.df[target_col].tail(n_samples)\n",
    "\n",
    "            print(f\"\\n{' Data Context ':=^50}\")\n",
    "            print(f\"Data Type: {'Time Series' if self.is_time_series else 'Standard Dataset'}\")\n",
    "            if self.is_time_series:\n",
    "                print(f\"Analyzing last {n_days} days of data\")\n",
    "            else:\n",
    "                print(f\"Analyzing last {n_samples} records\")\n",
    "\n",
    "            # Rest of the emergency calculation logic\n",
    "            target_col_lower = target_col.lower()\n",
    "            critical_direction = None\n",
    "\n",
    "            # Check existing context lists\n",
    "            if target_col_lower in self.context_lists['lower_is_better']:\n",
    "                critical_direction = 'above'\n",
    "                print(f\"Context: Known metric where lower values are better\")\n",
    "            elif target_col_lower in self.context_lists['higher_is_better']:\n",
    "                critical_direction = 'below'\n",
    "                print(f\"Context: Known metric where higher values are better\")\n",
    "            elif target_col_lower in self.context_lists['neutral']:\n",
    "                critical_direction = 'both'\n",
    "                print(f\"Context: Neutral metric monitoring both directions\")\n",
    "\n",
    "            # AI detection for unknown attributes\n",
    "            if not critical_direction and auto_detect_direction:\n",
    "                print(\"\\nPerforming AI context analysis...\")\n",
    "                critical_direction, confidence = self._detect_critical_direction(target_col)\n",
    "                print(f\"AI detected direction: {critical_direction} (confidence: {confidence:.0%})\")\n",
    "                \n",
    "                if confidence < 0.7:\n",
    "                    critical_direction, should_save = self._prompt_user_for_direction(target_col)\n",
    "                    if should_save:\n",
    "                        self._update_context_lists(target_col_lower, critical_direction)\n",
    "                if critical_direction == 'neutral':\n",
    "                    print(\"\\nAttribute marked as non-critical - no emergency level calculated\")\n",
    "                    return 1\n",
    "            # Set thresholds\n",
    "            if user_avg is not None:\n",
    "                threshold = user_avg * sensitivity\n",
    "                print(f\"\\nUsing custom average: {user_avg:.2f}\")\n",
    "                print(f\"Sensitivity threshold: ±{sensitivity:.0%} → ±{threshold:.2f}\")\n",
    "                \n",
    "                if critical_direction == 'above':\n",
    "                    upper_thresh = user_avg + threshold\n",
    "                    threshold_info = {'Upper Threshold': upper_thresh}\n",
    "                    critical_condition = recent_data > upper_thresh\n",
    "                elif critical_direction == 'below':\n",
    "                    lower_thresh = user_avg - threshold\n",
    "                    threshold_info = {'Lower Threshold': lower_thresh}\n",
    "                    critical_condition = recent_data < lower_thresh\n",
    "                else:\n",
    "                    lower_thresh = user_avg - threshold\n",
    "                    upper_thresh = user_avg + threshold\n",
    "                    threshold_info = {'Lower Threshold': lower_thresh, 'Upper Threshold': upper_thresh}\n",
    "                    critical_condition = (recent_data < lower_thresh) | (recent_data > upper_thresh)\n",
    "            else:\n",
    "                historical = self.df[target_col]\n",
    "                lower_quantile = historical.quantile(sensitivity)\n",
    "                upper_quantile = historical.quantile(1 - sensitivity)\n",
    "                print(f\"\\nUsing historical percentiles (sensitivity: {sensitivity:.0%})\")\n",
    "                print(f\"Historical range: {historical.min():.2f} - {historical.max():.2f}\")\n",
    "                \n",
    "                if critical_direction == 'above':\n",
    "                    threshold_info = {'Critical Threshold': upper_quantile}\n",
    "                    critical_condition = recent_data > upper_quantile\n",
    "                elif critical_direction == 'below':\n",
    "                    threshold_info = {'Critical Threshold': lower_quantile}\n",
    "                    critical_condition = recent_data < lower_quantile\n",
    "                else:\n",
    "                    threshold_info = {'Lower Threshold': lower_quantile, 'Upper Threshold': upper_quantile}\n",
    "                    critical_condition = (recent_data < lower_quantile) | (recent_data > upper_quantile)\n",
    "\n",
    "            # Display threshold information\n",
    "            print(\"\\nThreshold Configuration:\")\n",
    "            for name, value in threshold_info.items():\n",
    "                print(f\"- {name}: {value:.2f}\")\n",
    "\n",
    "            # Calculate emergency level\n",
    "            critical_count = critical_condition.sum()\n",
    "            proportion = critical_count / len(recent_data)\n",
    "            self.emergency_level = min(5, max(1, int(proportion * 5) + 1))\n",
    "\n",
    "            # Detailed results output\n",
    "            print(f\"\\nAnalysis Results:\")\n",
    "            print(f\"Critical values found: {critical_count}/{len(recent_data)} ({proportion:.0%})\")\n",
    "            print(f\"Emergency level calculation: ({critical_count}/{len(recent_data)}) * 5 + 1 = {self.emergency_level}\")\n",
    "            print(f\"{' Final Emergency Level ':=^50}\")\n",
    "            print(f\"Level {self.emergency_level}: \", end=\"\")\n",
    "            \n",
    "            level_descriptions = {\n",
    "                1: \"Normal - Minimal anomalies detected\",\n",
    "                2: \"Low Risk - Slightly elevated anomalies\",\n",
    "                3: \"Moderate - Significant anomalies present\",\n",
    "                4: \"High - Critical pattern emerging\",\n",
    "                5: \"Critical - Immediate attention required\"\n",
    "            }\n",
    "            print(level_descriptions.get(self.emergency_level, \"Unknown level\"))\n",
    "            \n",
    "            return self.emergency_level\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"Error calculating emergency level: {str(e)}\"\n",
    "\n",
    "    def _detect_critical_direction(self, target_col):\n",
    "        \"\"\"AI-powered direction detection with confidence scoring\"\"\"\n",
    "        analysis_prompt = f\"\"\"Analyze this metric context:\n",
    "        Column name: {target_col}\n",
    "        First values: {self.df[target_col].head().tolist()}\n",
    "        Recent trend: {self.df[target_col].tail(7).mean():.2f}\n",
    "        Historical average: {self.df[target_col].mean():.2f}\n",
    "\n",
    "        Should we alert for HIGH or LOW values? \n",
    "        Provide confidence (0-1). Format: HIGH/LOW/BOTH,CONFIDENCE\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = generate(\n",
    "                model='deepseek-r1:7b',\n",
    "                prompt=analysis_prompt,\n",
    "                stream=False\n",
    "            )\n",
    "            direction, confidence = response['response'].split(',')\n",
    "            direction_map = {'high': 'above', 'low': 'below', 'both': 'both'}\n",
    "            return direction_map[direction.strip().lower()], float(confidence)\n",
    "        except:\n",
    "            return None, 0.0\n",
    "\n",
    "    def _prompt_user_for_direction(self, target_col):\n",
    "        \"\"\"Interactive user prompt for new attributes with 'does not matter' option\"\"\"\n",
    "        print(f\"\\n⚠️ New attribute detected: '{target_col}'\")\n",
    "        print(f\"Sample values: {self.df[target_col].sample(5).tolist()}\")\n",
    "\n",
    "        while True:\n",
    "            print(\"\\nShould we alert for:\")\n",
    "            print(\"1. High values\")\n",
    "            print(\"2. Low values\")\n",
    "            print(\"3. Both directions\")\n",
    "            print(\"4. Does not matter (neutral)\")\n",
    "            choice = input(\"Choice (1-4): \").strip()\n",
    "\n",
    "            if choice == '1':\n",
    "                direction = 'above'\n",
    "                break\n",
    "            elif choice == '2':\n",
    "                direction = 'below'\n",
    "                break\n",
    "            elif choice == '3':\n",
    "                direction = 'both'\n",
    "                break\n",
    "            elif choice == '4':\n",
    "                direction = 'neutral'\n",
    "                break\n",
    "            print(\"Invalid choice. Please enter 1, 2, 3, or 4\")\n",
    "\n",
    "        if direction != 'neutral':\n",
    "            save = input(\"Remember this setting for future datasets? (y/n): \").lower() == 'y'\n",
    "        else:\n",
    "            save = False\n",
    "\n",
    "        return direction, save\n",
    "\n",
    "    def _update_context_lists(self, attribute, direction):\n",
    "        \"\"\"Store new attribute in appropriate context list\"\"\"\n",
    "        attribute = attribute.lower()\n",
    "\n",
    "        if direction == 'above':\n",
    "            self.context_lists['lower_is_better'].add(attribute)\n",
    "        elif direction == 'below':\n",
    "            self.context_lists['higher_is_better'].add(attribute)\n",
    "        elif direction == 'both':\n",
    "            self.context_lists['neutral'].add(attribute)\n",
    "        elif direction == 'neutral':\n",
    "            # Don't add to any list, treat as non-critical\n",
    "            pass\n",
    "\n",
    "        if direction != 'neutral':\n",
    "            self.learned_attributes[attribute] = direction\n",
    "            print(f\"✓ Learned new attribute: '{attribute}' -> {direction}\")\n",
    "        else:\n",
    "            print(f\"✓ Marked '{attribute}' as non-critical\")\n",
    "\n",
    "\n",
    "    def generate_forecast(self, target_col, periods=365):\n",
    "        \"\"\"Time series forecasting (only for time series data)\"\"\"\n",
    "        if not self.is_time_series:\n",
    "            return None, None, \"Forecasting requires time series data\"\n",
    "        \n",
    "        if self.date_column not in self.df.columns:\n",
    "            return None, None, \"No date column found for forecasting\"\n",
    "\n",
    "        # Prepare data for Prophet\n",
    "        df_prophet = self.df[[self.date_column, target_col]].rename(\n",
    "            columns={self.date_column: 'ds', target_col: 'y'})\n",
    "\n",
    "        # Train Prophet model\n",
    "        try:\n",
    "            model = Prophet()\n",
    "            model.fit(df_prophet)\n",
    "            future = model.make_future_dataframe(periods=periods)\n",
    "            forecast = model.predict(future)\n",
    "\n",
    "            # Save forecast plot\n",
    "            fig = model.plot(forecast)\n",
    "            forecast_plot = io.BytesIO()\n",
    "            fig.savefig(forecast_plot, format='png')\n",
    "            plt.close()\n",
    "\n",
    "            return forecast_plot.getvalue(), forecast.tail(periods), None\n",
    "        except Exception as e:\n",
    "            return None, None, f\"Forecasting error: {str(e)}\"\n",
    "\n",
    "    def analyze_entire_dataset(self, emergency_threshold=3):\n",
    "        \"\"\"\n",
    "        Perform comprehensive analysis of the entire dataset including:\n",
    "        - Basic statistics\n",
    "        - Missing values analysis\n",
    "        - Correlation matrix\n",
    "        - Emergency level overview\n",
    "        - Data type distribution\n",
    "        \"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        # Basic Dataset Overview\n",
    "        analysis['overview'] = {\n",
    "            'num_rows': self.df.shape[0],\n",
    "            'num_cols': self.df.shape[1],\n",
    "            'memory_usage': self.df.memory_usage(deep=True).sum() // 1024,  # KB\n",
    "            'duplicate_rows': self.df.duplicated().sum(),\n",
    "            'time_series': self.is_time_series\n",
    "        }\n",
    "\n",
    "        # Column Type Analysis\n",
    "        dtype_counts = self.df.dtypes.value_counts().to_dict()\n",
    "        analysis['dtype_distribution'] = {str(k): v for k, v in dtype_counts.items()}\n",
    "\n",
    "        # Missing Values Analysis\n",
    "        missing = self.df.isnull().sum()\n",
    "        analysis['missing_values'] = {\n",
    "            'total_missing': missing.sum(),\n",
    "            'columns_with_missing': missing[missing > 0].to_dict(),\n",
    "            'missing_percentage': (missing / len(self.df)).to_dict()\n",
    "        }\n",
    "\n",
    "        # Numerical Analysis\n",
    "        numerical_cols = self.df.select_dtypes(include=np.number).columns\n",
    "        analysis['numerical'] = {}\n",
    "        for col in numerical_cols:\n",
    "            analysis['numerical'][col] = {\n",
    "                'mean': self.df[col].mean(),\n",
    "                'median': self.df[col].median(),\n",
    "                'std': self.df[col].std(),\n",
    "                'min': self.df[col].min(),\n",
    "                'max': self.df[col].max(),\n",
    "                'skew': self.df[col].skew(),\n",
    "                'emergency_level': self.calculate_emergency_level(col, n_samples=len(self.df))\n",
    "            }\n",
    "\n",
    "        # Categorical Analysis\n",
    "        categorical_cols = self.df.select_dtypes(include='object').columns\n",
    "        analysis['categorical'] = {}\n",
    "        for col in categorical_cols:\n",
    "            analysis['categorical'][col] = {\n",
    "                'unique_values': self.df[col].nunique(),\n",
    "                'top_value': self.df[col].mode().iloc[0] if not self.df[col].empty else None,\n",
    "                'top_frequency': self.df[col].value_counts().iloc[0] if not self.df[col].empty else 0\n",
    "            }\n",
    "\n",
    "        # Correlation Analysis\n",
    "        if len(numerical_cols) > 1:\n",
    "            analysis['correlation'] = {\n",
    "                'pearson': self.df[numerical_cols].corr().to_dict(),\n",
    "                'top_correlations': self._get_top_correlations()\n",
    "            }\n",
    "\n",
    "        # Emergency Level Summary\n",
    "        critical_columns = [col for col in numerical_cols \n",
    "                          if analysis['numerical'][col]['emergency_level'] >= emergency_threshold]\n",
    "        analysis['alerts'] = {\n",
    "            'emergency_threshold': emergency_threshold,\n",
    "            'critical_columns': critical_columns,\n",
    "            'total_critical': len(critical_columns)\n",
    "        }\n",
    "\n",
    "        return analysis\n",
    "\n",
    "    def _get_top_correlations(self, n=5):\n",
    "        \"\"\"Get top positive and negative correlations\"\"\"\n",
    "        corr_matrix = self.df.select_dtypes(include=np.number).corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        top_pairs = upper.stack().sort_values(ascending=False).head(n).index.tolist()\n",
    "        \n",
    "        return [f\"{pair[0]} - {pair[1]}\" for pair in top_pairs]\n",
    "\n",
    "    def generate_comprehensive_report(self):\n",
    "        \"\"\"Generate human-readable report from full analysis\"\"\"\n",
    "        analysis = self.analyze_entire_dataset()\n",
    "        report = []\n",
    "\n",
    "        # Overview Section\n",
    "        report.append(\"=\"*50)\n",
    "        report.append(\" Comprehensive Dataset Analysis Report \")\n",
    "        report.append(\"=\"*50)\n",
    "        report.append(f\"\\nDataset Overview:\")\n",
    "        report.append(f\"- Rows: {analysis['overview']['num_rows']:,}\")\n",
    "        report.append(f\"- Columns: {analysis['overview']['num_cols']:,}\")\n",
    "        report.append(f\"- Memory Usage: {analysis['overview']['memory_usage']:,} KB\")\n",
    "        report.append(f\"- Duplicate Rows: {analysis['overview']['duplicate_rows']:,}\")\n",
    "        report.append(f\"- Time Series Data: {analysis['overview']['time_series']}\")\n",
    "\n",
    "        # Data Types\n",
    "        report.append(\"\\nData Type Distribution:\")\n",
    "        for dtype, count in analysis['dtype_distribution'].items():\n",
    "            report.append(f\"- {dtype}: {count} columns\")\n",
    "\n",
    "        # Missing Values\n",
    "        report.append(\"\\nMissing Values Analysis:\")\n",
    "        report.append(f\"- Total Missing Values: {analysis['missing_values']['total_missing']:,}\")\n",
    "        if analysis['missing_values']['columns_with_missing']:\n",
    "            report.append(\"- Columns with Missing Values:\")\n",
    "            for col, count in analysis['missing_values']['columns_with_missing'].items():\n",
    "                pct = analysis['missing_values']['missing_percentage'][col] * 100\n",
    "                report.append(f\"  - {col}: {count:,} ({pct:.1f}%)\")\n",
    "        else:\n",
    "            report.append(\"- No missing values found\")\n",
    "\n",
    "        # Numerical Analysis\n",
    "        report.append(\"\\nNumerical Columns Summary:\")\n",
    "        for col, stats in analysis['numerical'].items():\n",
    "            report.append(f\"\\n[{col}]\")\n",
    "            report.append(f\"- Emergency Level: {stats['emergency_level']}/5\")\n",
    "            report.append(f\"- Range: {stats['min']:.2f} to {stats['max']:.2f}\")\n",
    "            report.append(f\"- Mean: {stats['mean']:.2f} | Median: {stats['median']:.2f}\")\n",
    "            report.append(f\"- Std Dev: {stats['std']:.2f} | Skew: {stats['skew']:.2f}\")\n",
    "\n",
    "        # Categorical Analysis\n",
    "        if analysis['categorical']:\n",
    "            report.append(\"\\nCategorical Columns Summary:\")\n",
    "            for col, stats in analysis['categorical'].items():\n",
    "                report.append(f\"\\n[{col}]\")\n",
    "                report.append(f\"- Unique Values: {stats['unique_values']:,}\")\n",
    "                report.append(f\"- Most Common: '{stats['top_value']}' ({stats['top_frequency']:,} occurrences)\")\n",
    "\n",
    "        # Correlation Analysis\n",
    "        if 'correlation' in analysis:\n",
    "            report.append(\"\\nTop Correlations:\")\n",
    "            for pair in analysis['correlation']['top_correlations']:\n",
    "                report.append(f\"- {pair}\")\n",
    "\n",
    "        # Critical Alerts\n",
    "        report.append(\"\\n\" + \"=\"*50)\n",
    "        report.append(\" Critical Alerts Summary \")\n",
    "        report.append(\"=\"*50)\n",
    "        if analysis['alerts']['total_critical'] > 0:\n",
    "            report.append(f\"\\n{analysis['alerts']['total_critical']} columns requiring attention (emergency ≥{analysis['alerts']['emergency_threshold']}):\")\n",
    "            for col in analysis['alerts']['critical_columns']:\n",
    "                report.append(f\"- {col} (Level {analysis['numerical'][col]['emergency_level']})\")\n",
    "        else:\n",
    "            report.append(\"\\nNo critical columns detected at current threshold\")\n",
    "\n",
    "        # Return the report as a single string\n",
    "        return \"\\n\".join(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731568d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CSVAnalyst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     analyst \u001b[38;5;241m=\u001b[39m CSVAnalyst()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load your CSV file\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolar_data_khulna_from_jan_2014_to_nov_2022.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CSVAnalyst' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    analyst = CSVAnalyst()\n",
    "    \n",
    "    # Load your CSV file\n",
    "    file_path = \"solar_data_khulna_from_jan_2014_to_nov_2022.csv\"\n",
    "    load_status = analyst.load_csv(file_path)\n",
    "    \n",
    "    if load_status is True:\n",
    "        # Generate automatic analysis\n",
    "        print(\"Basic Analysis:\\n\", analyst.analysis_report)\n",
    "        \n",
    "        # Generate visualizations\n",
    "        plots = analyst.generate_plots()\n",
    "        for name, plot_data in plots.items():\n",
    "            with open(f\"{name}.png\", \"wb\") as f:\n",
    "                f.write(plot_data)\n",
    "        \n",
    "        # Calculate emergency level\n",
    "        emergency_level = analyst.calculate_emergency_level(\n",
    "            #input\n",
    "            target_col=\"Temperature\",\n",
    "            n_days=30,\n",
    "            user_avg=0\n",
    "        )\n",
    "        print(f\"Emergency Level: {emergency_level}\")\n",
    "\n",
    "        # Generate forecast if time series\n",
    "        if analyst.is_time_series:\n",
    "            forecast_plot, forecast_data, error = analyst.generate_forecast(\n",
    "                #input\n",
    "                target_col=\"Temperature\",\n",
    "                periods=90\n",
    "            )\n",
    "            if error:\n",
    "                print(f\"Forecast Error: {error}\")\n",
    "            else:\n",
    "                forecast_data.to_csv(\"forecast_results.csv\")\n",
    "                with open(\"forecast_plot.png\", \"wb\") as f:\n",
    "                    f.write(forecast_plot)\n",
    "    else:\n",
    "        print(load_status)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03106e23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
